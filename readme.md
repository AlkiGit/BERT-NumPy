# Minimal BERT Implementation (Pure NumPy)

![Python](https://img.shields.io/badge/Python-3.8%2B-blue?style=flat-square&logo=python)
![NumPy](https://img.shields.io/badge/Library-NumPy%20Only-orange?style=flat-square&logo=numpy)
![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)

> **Caravan Research Paper Study Team - Alki**
>
> *An educational implementation of BERT built from scratch using only NumPy, co-developed with GPT-4o.*
>
> *GPT-4oì˜ ë„ì›€ì„ ë°›ì•„ ì˜¤ì§ NumPyë§Œì„ ì‚¬ìš©í•˜ì—¬ ë°”ë‹¥ë¶€í„°(Scratch) êµ¬í˜„í•œ êµìœ¡ìš© BERT í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.*

---

## ğŸ“– Project Overview (í”„ë¡œì íŠ¸ ê°œìš”)

This repository contains a minimalist implementation of **BERT (Bidirectional Encoder Representations from Transformers)**. Unlike standard implementations relying on deep learning frameworks like PyTorch or TensorFlow, this project builds the entire architectureâ€”including the automatic differentiation (autograd) engineâ€”using **only NumPy**.

This project aims to provide a deep understanding of the mathematical principles behind the Transformer architecture.

ì´ ì €ì¥ì†ŒëŠ” ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ ì—†ì´ **ìˆœìˆ˜ NumPy**ë§Œìœ¼ë¡œ êµ¬í˜„í•œ BERT ëª¨ë¸ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤. ìë™ ë¯¸ë¶„(Autograd) ì—”ì§„ë¶€í„° ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ê¹Œì§€ ì§ì ‘ êµ¬í˜„í•˜ì—¬ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ë‚´ë¶€ ë™ì‘ ì›ë¦¬ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤.

### âš ï¸ Note
* **Computation:** CPU Only (Optimized for educational clarity, not speed).
* **Dependency:** Zero external DL libraries (No Torch, No TF).

---

## ğŸ“‚ Repository Structure (í´ë” êµ¬ì¡°)

```bash
AlkiGit/
â”œâ”€â”€ computation/        # Custom Autograd Engine & Tensor Operations
â”‚                       # (ìë™ ë¯¸ë¶„ ë° í…ì„œ ì—°ì‚° ëª¨ë“ˆ)
â”œâ”€â”€ dataset/            # Data Loading & Preprocessing Utilities
â”‚                       # (ë°ì´í„° ë¡œë” ë° ì „ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°)
â”œâ”€â”€ model/              # BERT Architecture (Layers, Encoder, Attention)
â”‚                       # (BERT ëª¨ë¸ ì•„í‚¤í…ì²˜ êµ¬í˜„ì²´)
â”œâ”€â”€ tokenization/       # WordPiece Tokenizer Implementation
â”‚                       # (WordPiece í† í¬ë‚˜ì´ì €)
â”œâ”€â”€ bert_model.npz      # Pre-trained Model Weights (NumPy Archive)
â”‚                       # (ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼)
â”œâ”€â”€ train.py            # Main Training Entrypoint
â”‚                       # (í•™ìŠµ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸)
â””â”€â”€ LICENSE             # MIT License
```

---

## âœ¨ Key Features (í•µì‹¬ ê¸°ëŠ¥)

* **ğŸš« No External Frameworks:** Pure Python & NumPy implementation. (ì™¸ë¶€ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ ë¯¸ì‚¬ìš©)
* **âš™ï¸ Custom Autograd:** Lightweight reverse-mode automatic differentiation engine. (ì§ì ‘ êµ¬í˜„í•œ ìë™ ë¯¸ë¶„ ì—”ì§„)
* **ğŸ”¤ WordPiece Tokenizer:** Custom subword tokenization logic. (WordPiece í† í¬ë‚˜ì´ì € ë‚´ì¥)
* **ğŸ§  BERT Components:**
    * Multi-Head Self Attention
    * Layer Normalization & Residual Connections
    * Feed-Forward Networks
    * GELU Activation
* **ğŸ’¾ Model Persistence:** Save and load weights using `.npz` format. (ê°€ì¤‘ì¹˜ ì €ì¥ ë° ë¡œë“œ ì§€ì›)

---

## ğŸš€ Getting Started (ì‹œì‘í•˜ê¸°)

### 1. Prerequisites
All you need is Python and NumPy.

```bash
pip install numpy
```

### 2. Training (í•™ìŠµí•˜ê¸°)
To start training the model from scratch using the provided script:

```bash
python train.py
```

### 3. Loading Pre-trained Weights (ê°€ì¤‘ì¹˜ ë¡œë“œ)
You can load the included `bert_model.npz` to test the model without training.

```python
import numpy as np

# Load the weights
data = np.load('bert_model.npz')

# List all layers/weights stored
print(data.files)
```

---

## ğŸ‘¨â€ğŸ’» Developed By

**Caravan Research Paper Study Team - Alki**

This project serves as study material for:
* Computational Graphs & Backpropagation
* Matrix Calculus in Deep Learning
* Transformer Attention Mechanisms

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
